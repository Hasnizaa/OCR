{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNAIwaQ2r6xf54YbXO3ZkTa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hasnizaa/OCR/blob/main/DSP1_(data_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SJhC_5FRfasX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!pip install tesseract\n"
      ],
      "metadata": {
        "id": "PyLE-gEWqebb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "WO2nzufwTA4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!sudo apt install libtesseract-dev"
      ],
      "metadata": {
        "id": "r1K_IewmYo4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "KYPJ-TfWbgzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfE9u3YNfE1L"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "try:\n",
        " from PIL import Image\n",
        "except ImportError:\n",
        " import Image\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the Tesseract executable for Google Colab\n",
        "# Use the correct path which is usually /usr/bin/tesseract in Colab\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'"
      ],
      "metadata": {
        "id": "lx_ftNfVtvNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the above path doesn't work, try this alternative:\n",
        "#pytesseract.pytesseract.tesseract_cmd = r'/usr/local/bin/tesseract'"
      ],
      "metadata": {
        "id": "LpiT75bZWFfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Tesseract version to ensure it's installed and accessible\n",
        "print(pytesseract.get_tesseract_version())"
      ],
      "metadata": {
        "id": "AdBzgLg2WN2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "# List of image file paths\n",
        "image_files = [\n",
        "    'data3.jpg',\n",
        "    'data4(1).jpg',\n",
        "    'data5.jpg',\n",
        "    'data6.jpg',\n",
        "    'data7.jpg',\n",
        "    'data8.jpg',\n",
        "    'data9(1).jpg',\n",
        "    'data10(1).jpg',\n",
        "    'data11.jpg',\n",
        "    'data12.jpg',\n",
        "    'data13.jpg',\n",
        "    'data14.jpg',\n",
        "    'data15.jpg',\n",
        "    'data16.jpg'\n",
        "]\n",
        "\n",
        "# List to store extracted text from each image\n",
        "extracted_texts = []\n",
        "\n",
        "# Iterate over the list of image files\n",
        "for img_path in image_files:\n",
        "    # Open the image file\n",
        "    img = Image.open(img_path)\n",
        "\n",
        "    # Perform OCR on the image\n",
        "    text = pytesseract.image_to_string(img)\n",
        "\n",
        "    # Append the extracted text to the list\n",
        "    extracted_texts.append((img_path, text))\n",
        "\n",
        "# Print the extracted texts\n",
        "for img_path, text in extracted_texts:\n",
        "    print(f\"Text from {img_path}:\\n{text}\\n\")"
      ],
      "metadata": {
        "id": "pQioPEYDDvuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Define the pattern to extract transaction details, allowing for optional merchant name\n",
        "transaction_pattern = re.compile(\n",
        "    r'(\\d{2}/\\d{2}/\\d{2})\\s+\\|\\s+(.*?)\\s+([+-]?\\d{1,3}(,\\d{3})*(\\.\\d+)?)([+-])?\\s+([\\d,.]+)(\\n([A-Za-z\\s]+))?'\n",
        ")\n",
        "\n",
        "# Initialize a list to store extracted transactions\n",
        "transactions = []\n",
        "\n",
        "# Process each extracted text entry\n",
        "for img_path, extracted_text in extracted_texts:\n",
        "    # Use regex to find all matches\n",
        "    for match in transaction_pattern.finditer(extracted_text):\n",
        "        entry_date, description, amount, _, _, sign, balance, _, merchant = match.groups()\n",
        "\n",
        "        # Clean up the extracted data\n",
        "        amount = amount.replace(',', '')\n",
        "        balance = balance.replace(',', '')\n",
        "\n",
        "        # Determine debit or credit\n",
        "        debit = 0.0\n",
        "        credit = 0.0\n",
        "        if sign == '+':\n",
        "            credit = float(amount)\n",
        "        elif sign == '-':\n",
        "            debit = float(amount)\n",
        "        else:  # If no sign, assume debit\n",
        "            debit = float(amount)\n",
        "\n",
        "        # Notes: Any additional description from the main description field\n",
        "        notes = description.strip()\n",
        "\n",
        "        # If there's no merchant name captured, set it to \"UNKNOWN\"\n",
        "        merchant = merchant.strip() if merchant else \"UNKNOWN\"\n",
        "\n",
        "        # Append transaction details\n",
        "        transactions.append([entry_date, merchant, notes, debit, credit, float(balance)])\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(transactions, columns=['Date', 'Merchant', 'PaymentType', 'Debit', 'Credit', 'Balance'])\n",
        "\n",
        "# Display the DataFrame\n",
        "df\n"
      ],
      "metadata": {
        "id": "HoBmdt0Ca5y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image1 = 'data1.jpg'\n",
        "image2 = 'data2.jpg'"
      ],
      "metadata": {
        "id": "4CPmxPEp4Aa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform OCR on the preprocessed image\n",
        "extractedInformation1 = pytesseract.image_to_string(image1)\n",
        "print(extractedInformation1)"
      ],
      "metadata": {
        "id": "QOFZUNfbzHBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform OCR on the preprocessed image\n",
        "extractedInformation2 = pytesseract.image_to_string(image2)\n",
        "print(extractedInformation2)"
      ],
      "metadata": {
        "id": "lofqGodBk-Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert to csv file"
      ],
      "metadata": {
        "id": "6H5QSNvBipnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Define the pattern to extract transaction details, allowing for optional merchant name\n",
        "transaction_pattern = re.compile(\n",
        "    r'(\\d{2}/\\d{2}/\\d{2})\\s+\\|\\s+(.*?)\\s+([+-]?\\d{1,3}(,\\d{3})*(\\.\\d+)?)([+-])?\\s+([\\d,.]+)(\\n([A-Za-z\\s]+))?'\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize a list to store extracted transactions\n",
        "transactions = []\n",
        "\n",
        "# Use regex to find all matches\n",
        "for match in transaction_pattern.finditer(extractedInformation1):\n",
        "    entry_date, description, amount, _, _, sign, balance, _, merchant = match.groups()\n",
        "\n",
        "    # Clean up the extracted data\n",
        "    amount = amount.replace(',', '')\n",
        "    balance = balance.replace(',', '')\n",
        "\n",
        "    # Determine debit or credit\n",
        "    debit = 0.0\n",
        "    credit = 0.0\n",
        "    if sign == '+':\n",
        "        credit = float(amount)\n",
        "    elif sign == '-':\n",
        "        debit = float(amount)\n",
        "    else:  # If no sign, assume debit\n",
        "        debit = float(amount)\n",
        "\n",
        "    # Notes: Any additional description from the main description field\n",
        "    notes = description.strip()\n",
        "\n",
        "    # If there's no merchant name captured, set it to \"UNKNOWN\"\n",
        "    merchant = merchant.strip() if merchant else \"UNKNOWN\"\n",
        "\n",
        "    # Append transaction details\n",
        "    transactions.append([entry_date, merchant, notes, debit, credit, float(balance)])\n",
        "\n",
        "# Create a DataFrame\n",
        "df1 = pd.DataFrame(transactions, columns=['Date', 'Merchant', 'PaymentType', 'Debit', 'Credit', 'Balance'])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df1)\n"
      ],
      "metadata": {
        "id": "INC479hLkXM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Define the pattern to extract transaction details, allowing for optional merchant name\n",
        "transaction_pattern = re.compile(\n",
        "    r'(\\d{2}/\\d{2}/\\d{2})\\s+\\|\\s+(.*?)\\s+([+-]?\\d{1,3}(,\\d{3})*(\\.\\d+)?)([+-])?\\s+([\\d,.]+)(\\n([A-Za-z\\s]+))?'\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize a list to store extracted transactions\n",
        "transactions = []\n",
        "\n",
        "# Use regex to find all matches\n",
        "for match in transaction_pattern.finditer(extractedInformation2):\n",
        "    entry_date, description, amount, _, _, sign, balance, _, merchant = match.groups()\n",
        "\n",
        "    # Clean up the extracted data\n",
        "    amount = amount.replace(',', '')\n",
        "    balance = balance.replace(',', '')\n",
        "\n",
        "    # Determine debit or credit\n",
        "    debit = 0.0\n",
        "    credit = 0.0\n",
        "    if sign == '+':\n",
        "        credit = float(amount)\n",
        "    elif sign == '-':\n",
        "        debit = float(amount)\n",
        "    else:  # If no sign, assume debit\n",
        "        debit = float(amount)\n",
        "\n",
        "    # Notes: Any additional description from the main description field\n",
        "    notes = description.strip()\n",
        "\n",
        "    # If there's no merchant name captured, set it to \"UNKNOWN\"\n",
        "    merchant = merchant.strip() if merchant else \"UNKNOWN\"\n",
        "\n",
        "    # Append transaction details\n",
        "    transactions.append([entry_date, merchant, notes, debit, credit, float(balance)])\n",
        "\n",
        "# Create a DataFrame\n",
        "df2 = pd.DataFrame(transactions, columns=['Date', 'Merchant', 'PaymentType', 'Debit', 'Credit', 'Balance'])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df2)\n"
      ],
      "metadata": {
        "id": "Vd9v5Oloq_GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the DataFrames\n",
        "merged_df = pd.concat([df1, df2, df], ignore_index=True)\n",
        "\n",
        "merged_df"
      ],
      "metadata": {
        "id": "Q2QzTHz_NhkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.isna().sum()"
      ],
      "metadata": {
        "id": "sNbRoe_lf_3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
        "merged_df"
      ],
      "metadata": {
        "id": "MgII-cT-ir_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payment_types = merged_df['PaymentType']\n",
        "payment_types.unique()"
      ],
      "metadata": {
        "id": "-C1uKktKkz7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of replacements\n",
        "payment_type_replacements = {\n",
        "    'SNHO123': 'Stock Transaction',\n",
        "    'SNHO12)': 'Stock Transaction',\n",
        "    'SALE DEBIT': 'Card Payment',\n",
        "    'FPX PAYMENT FR A/': 'Online Payment',\n",
        "    'IBK FUND TFR FR A/C': 'Fund Transfer In (IBK)',\n",
        "    'FUND TRANSFER TO A/': 'Fund Transfer Out',\n",
        "    'PAYMENT VIA MYDEBIT': 'MyDebit Payment',\n",
        "    'TRANSFER FROM A/C': 'Transfer In',\n",
        "    'SVG GIRO CR': 'Giro Credit',\n",
        "    'PRE-AUTH MYDEBIT': 'MyDebit Pre-Authorization',\n",
        "    'REV PREAUTH MYDEBIT': 'MyDebit Pre-Auth Reversal',\n",
        "    'IBK FUND TFR FRA/C': 'Fund Transfer Out (IBK) ',\n",
        "    'CASH WITHDRAWAL': 'Cash Withdrawal',\n",
        "    'FUND TRANSFER TO': 'Fund Transfer Out',\n",
        "    'MAS PAYMENT CREDIT': 'MAS Credit',\n",
        "    'PYMT FROM A/C': 'Payment from Account',\n",
        "    'IBK FUND TFR TO A/C': 'Fund Transfer Out (IBK)',\n",
        "    'FPOPRTREMETEAPO': 'Payment from Account',\n",
        "    'pviatstbpPA700100': 'Online Payment'\n",
        "}\n",
        "\n",
        "# Apply the replacements to the 'PaymentType' column\n",
        "merged_df['PaymentType'] = merged_df['PaymentType'].replace(payment_type_replacements)\n",
        "merged_df"
      ],
      "metadata": {
        "id": "V-sf-JKvrEND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merchantname = merged_df['Merchant']\n",
        "merchantname.unique()"
      ],
      "metadata": {
        "id": "SNBexTxjlCYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of replacements\n",
        "merchant_replacements = {\n",
        "    'MALACCA SECURITIES S\\nWC': 'Malacca Securities',\n",
        "    'T': 'IPAY(88) SDN BHD',\n",
        "    'cuti': 'UNKNOWN',\n",
        "    'Perhation': 'UNKNOWN',\n",
        "    'Perhetion': 'UNKNOWN',\n",
        "    'MALACCA SECURITIES S\\n\\nWC': 'Malacca Securities',\n",
        "    'MCL': 'Malacca Securities',\n",
        "    'TTI NURUL HAIDAR B': 'SITI NURUL HAIDAR B',\n",
        "    'MALACCA SECURITIES': 'Malacca Securities',\n",
        "    'SITI NURUL': 'UNKNOWN',\n",
        "    'PUAN SITI NURUL HAT': 'UNKNOWN',\n",
        "    'ITI NURUL IDAYU BI': 'SITI NURUL IDAYU BI',\n",
        "    'MALACCA SECURITIES S\\nwe': 'Malacca Securities'\n",
        "}\n",
        "\n",
        "# Apply the replacements to the 'PaymentType' column\n",
        "merged_df['Merchant'] = merged_df['Merchant'].replace(merchant_replacements)\n",
        "merged_df"
      ],
      "metadata": {
        "id": "j8YW2_KqtTgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update online payment for payment type for foodpanda\n",
        "merged_df.loc[merged_df['PaymentType'] == 'Online Payment', 'Merchant'] = 'FOODPANDA MALAYSIA'\n",
        "merged_df"
      ],
      "metadata": {
        "id": "6fvFYTPWhxix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.loc[merged_df['PaymentType'] == 'Stock Transaction', 'Merchant'] = 'Malacca Securities'\n",
        "merged_df"
      ],
      "metadata": {
        "id": "x6v4yc6KyerD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update fpx payment for payment type for foodpanda\n",
        "merged_df.loc[merged_df['PaymentType'] == 'MyDebit Payment', 'Merchant'] = 'UNKNOWN'"
      ],
      "metadata": {
        "id": "G_h7-J6Yywkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update fpx payment for payment type for foodpanda\n",
        "merged_df.loc[merged_df['PaymentType'] == 'Cash Withdrawal', 'Merchant'] = 'UNKNOWN'"
      ],
      "metadata": {
        "id": "0mS72nlxy-W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.loc[merged_df['PaymentType'] == 'MyDebit Pre-Authorization', 'Merchant'] = 'PETRON SETIA ALAM'\n"
      ],
      "metadata": {
        "id": "MqCjev0zzIyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.loc[merged_df['PaymentType'] == 'MyDebit Pre-Auth Reversal', 'Merchant'] = 'PETRON SETIA ALAM'"
      ],
      "metadata": {
        "id": "VWd2vkjTzTgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your DataFrame is named 'df'\n",
        "merged_df.to_csv('DATA.csv', index=False)"
      ],
      "metadata": {
        "id": "rHoBdw0shRmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Time Series Plot (Date vs. Debit/Credit/Balance) - July, August, September\n",
        "import matplotlib.pyplot as plt  # Import the matplotlib.pyplot module and assign it the alias 'plt'\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "\n",
        "# Filter the DataFrame for July, August, and September\n",
        "filtered_df = merged_df[\n",
        "    (merged_df['Date'].dt.month >= 7) & (merged_df['Date'].dt.month <= 9)\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(filtered_df['Date'], filtered_df['Debit'], label='Debit', marker='o')\n",
        "plt.plot(filtered_df['Date'], filtered_df['Credit'], label='Credit', marker='o')\n",
        "plt.title(\n",
        "    'Time Series Plot: Debit and Credit  Over Time (July, August, September)'\n",
        ")\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Amount')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3cySXHJJ0FKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monthly Spending Trend (Debit) limited to months 7, 8, and 9\n",
        "merged_df['Month'] = merged_df['Date'].dt.month  # Extract the month as an integer\n",
        "df_filtered = merged_df[merged_df['Month'].isin([7, 8, 9])]  # Filter only months 7, 8, and 9\n",
        "\n",
        "# Group by month and sum the debit transactions\n",
        "monthly_spending = df_filtered.groupby('Month')['Debit'].sum()\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "monthly_spending.plot(kind='line', marker='o', color='blue')\n",
        "plt.title('Monthly Spending Trend (Debit) - Months 7, 8, and 9')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Debit Amount')\n",
        "plt.xticks([7, 8, 9], ['July', 'August', 'September'])  # Custom x-ticks for months\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "VMvY_z3jwvEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merchant analysis\n",
        "\n",
        "top_merchants = merged_df.groupby('Merchant')['Debit'].sum().sort_values(ascending=False).head(10)\n",
        "top_merchants.plot(kind='bar', figsize=(10, 6), title='Top 10 Merchants by Spending')\n",
        "plt.xlabel('Merchant')\n",
        "plt.ylabel('Total Debit')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7fgIZRM7jw7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Transaction Count by PaymentType - Top 10\n",
        "import matplotlib.pyplot as plt  # Import the matplotlib.pyplot module and assign it the alias 'plt'\n",
        "\n",
        "payment_type_count = merged_df['PaymentType'].value_counts().head(10)  # Get top 10\n",
        "\n",
        "plt.figure(figsize=(10, 6))  # Adjust figure size for better visibility\n",
        "payment_type_count.plot(kind='bar', color=['skyblue', 'lightgreen'])\n",
        "plt.title('Transaction Count by PaymentType (Top 10)')\n",
        "plt.xlabel('Payment Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5dVQtz5Z0a-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFM AND K-MEANS"
      ],
      "metadata": {
        "id": "CqP0-IX_v-el"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fE4Fsrka0niU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate RFM metrics\n",
        "snapshot_date = merged_df['Date'].max() + pd.Timedelta(days=1)\n",
        "rfm = merged_df.groupby('Merchant').agg({\n",
        "    'Date': lambda x: (snapshot_date - x.max()).days,  # Recency\n",
        "    'PaymentType': 'count',                             # Frequency\n",
        "    'Debit': 'sum'                                   # Monetary (assuming 'debit' represents spend)\n",
        "}).reset_index()\n",
        "rfm.rename(columns={\n",
        "    'Date': 'Recency',\n",
        "    'PaymentType': 'Frequency',\n",
        "    'Debit': 'Monetary'\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "-KKcp6Z90ois"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Prepare RFM data (only for known merchants) for clustering\n",
        "rfm_features = rfm[['Recency', 'Frequency', 'Monetary']]"
      ],
      "metadata": {
        "id": "vl0oz6PAgl-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler  # Import MinMaxScaler\n",
        "\n",
        "# Initialize MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the RFM features\n",
        "rfm_scaled = scaler.fit_transform(rfm_features)"
      ],
      "metadata": {
        "id": "JhPm6Tg7mTNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Normalize the RFM data\n",
        "scaler = StandardScaler()\n",
        "rfm_scaled = scaler.fit_transform(rfm_features)"
      ],
      "metadata": {
        "id": "SGd9Zsm_gpQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-Means\n",
        "# Determine the optimal number of clusters using the Elbow Method\n",
        "inertia = []\n",
        "# Change the range of k_values to be less than or equal to the number of samples\n",
        "k_values = range(1, 5)  # Assuming rfm_normalized has 8 samples or less\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(rfm_scaled)\n",
        "    inertia.append(kmeans.inertia_)"
      ],
      "metadata": {
        "id": "lD7z0vV10zQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the Elbow Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, inertia, marker='o')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4WtfejZ402br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit K-Means with optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)"
      ],
      "metadata": {
        "id": "cZuuyAmF07eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rfm)"
      ],
      "metadata": {
        "id": "b8zBmnr6eYDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Reduce dimensionality to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "rfm_pca = pca.fit_transform(rfm_scaled)\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(rfm_pca[:, 0], rfm_pca[:, 1], c=rfm['Cluster'], cmap='viridis', s=50, alpha=0.7)\n",
        "\n",
        "# Plot the centroids of the clusters\n",
        "centroids = pca.transform(kmeans.cluster_centers_)\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('K-Means Clustering Visualization')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vjFpins21HX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the clusters, selecting only numeric columns for the mean calculation\n",
        "cluster_summary = rfm.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean()\n",
        "print(cluster_summary)"
      ],
      "metadata": {
        "id": "pYbdwe-I1LD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Evaluate the model using Silhouette Score\n",
        "# Get the cluster labels and the data used for clustering\n",
        "labels = rfm['Cluster']\n",
        "data_for_clustering = rfm[['Recency', 'Frequency', 'Monetary']]\n",
        "\n",
        "# Calculate the Silhouette Score\n",
        "silhouette_avg = silhouette_score(data_for_clustering, labels)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")"
      ],
      "metadata": {
        "id": "tdkQ3Dlc1bpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "anomaly detection"
      ],
      "metadata": {
        "id": "OtEzqWEw1mAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest"
      ],
      "metadata": {
        "id": "V2O48Rq-1pRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numeric columns for scaling\n",
        "merged_df_numeric = merged_df.select_dtypes(include=[float, int])  # Select only numeric columns"
      ],
      "metadata": {
        "id": "T8jADJfw1s4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "df_normalized = scaler.fit_transform(merged_df_numeric)  # Normalize the features"
      ],
      "metadata": {
        "id": "0Cnnecp91vaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Isolation Forest model\n",
        "iso_forest = IsolationForest(contamination=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "s_BkQMZm1yFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model and predict anomalies\n",
        "merged_df['Anomaly'] = iso_forest.fit_predict(df_normalized)"
      ],
      "metadata": {
        "id": "nfVSBiF-10Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Map the predictions: -1 = anomaly, 1 = normal\n",
        "merged_df['Anomaly'] = merged_df['Anomaly'].map({1: 'Normal', -1: 'Anomaly'})"
      ],
      "metadata": {
        "id": "KT4gfLHv12bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Reduce dimensionality to 2D using PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "df_pca = pca.fit_transform(df_normalized)"
      ],
      "metadata": {
        "id": "F9rerlkf14n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df_pca[:, 0], df_pca[:, 1], c=(merged_df['Anomaly'] == 'Anomaly'), cmap='coolwarm', s=50, alpha=0.7)\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Isolation Forest Anomaly Detection')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nOB3NVrp18ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Silhouette Score\n",
        "# Assuming 'Anomaly' column in merged_df contains anomaly predictions (Normal/Anomaly)\n",
        "# Map 'Anomaly' column to numeric values (e.g., 1 for Anomaly, 0 for Normal)\n",
        "predicted_anomalies = merged_df['Anomaly'].map({'Anomaly': 1, 'Normal': 0})\n",
        "silhouette_avg = silhouette_score(df_normalized, predicted_anomalies)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")"
      ],
      "metadata": {
        "id": "h61MVHxRl-sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "xRP4KHEclj8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}